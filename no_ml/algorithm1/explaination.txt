This document explains the reasoning behind the first traceroute algorithm...

Parsing the traceroute data:
- The last 5 ip addresses (ignoring the last byte of data) is recorded. 
- For the time being the latency, time to live, and delay are all ignored.

Training of traceroute data:
- only includes traceroutes that consist of 30% or more of all the data in
    Training data set. This algorithm protects against outliers in the
    trained set of traceroutes

Testing of tracerouet data:
- Tests each traceroute against the training set. If there are three or more
    ips in the test traceroute that are also in any of the trained traceroutes, 
    the test traceroute passes. If 10% or more of the traceroutes do not pass, 
    the device is predicted to have moved

Notes:
- This algorithm struggles, for some ip addresses (such as salt lake) it has many
    similarities to other ip addresses farther away (such as boise, alabama, etc.)
    Since the algorithm includes the total score of ip addresses in all of the 
    training traceroutes it does not protect strongly enough against these similarities
    with differing locations
- For other ip addresses such as 136.26.62.167, the ip addresses change significantly
    quite frequently (I'm assuming this is due to load balancing), The algorithm's 
    threshold for ip addresses in the trained traceroutes is too high and cannot
    be completed. Once a significantly different and new traceroute appears for this
    ip address, the algorithm assumes that it has moved and stops.

